# 计算机网络

## 2

* **分布式科学计算**：P2P技术可以使得众多终端的CPU资源联合起来，服务于一个共同的计算。这种计算一般是计算量巨大、数据极多、耗时很长的科学计算。在每次计算过程中，任务(包括逻辑与数据等)被划分成多个片，被分配到参与科学计算的P2P节点机器上。在不影响原有计算机使用的前提下，人们利用分散的CPU资源完成计算任务，并将结果返回给一个或多个服务器，将众多结果进行整合，以得到最终结果。
* **文件共享**：BitTorrent是运用P2P技术进行文件共享的典型应用。BitTorrent中的节点在共享一个文件时，首先将文件分片并将文件和分片信息保存在一个流(Torrent)类型文件中，这种节点被形象地称作“种子”节点。其他用户在下载该文件时根据Torrent文件的信息，将文件的部分分片下载下来，然后在其他下载该文件的节点之间共享自己已经下载的分片，互通有无，从而实现文件的快速分发。
* **通讯与信息共享**：即使用*IP*及*P2P*技术，在互联网上进行高质量、免费通话或信息共享。其代表软件是QQ、Skype，相信很多人已使用过。通过P2P技术实现的通讯应用，我们可以与全球各地的人交谈、聆听语音、观看视频、使用文字交流，再无需考虑通信费用、距离或时间。
* **视频直播**：在P2P技术实现的流媒体视频直播应用中，每个节点通过登录服务器进入网络，并得到一些邻居列表。每个节点维护一定数量的邻居成员，并从中选出最合适的“伙伴”节点与之交换数据。这样的应用适合用户量较大的视频直播场景，其带宽成本相较其他视频直播应用也具有较大的优势。

## 3

在这里我们设计一种基于P2P网络的流媒体应用系统。该流媒体应用利用自动式文件分块重组的方案，利用P2P网络的优势，可以做到在下载的同时播放视频。同时该应用的多点下载的特点可以保证用户视频播放的完整性和流畅性。

基于P2P的设计策略，所有该应用的用户将组成一个逻辑上的网络，其自治、稳定、可扩展。各个用户节点对等，既可以方便地获取网络中的流媒体资源，也可以随时上传分享。

在P2P网络的架构之外，我们还将给该应用设计一个中心的服务器，用以保存流媒体资源索引信息，以及记录各个用户的cookie信息。从而使用户可以方便的查询自己需要的资源，并且可以实现包括用户收藏，用户喜好推荐等一系列提升用户体验的功能。

***

## 4

Overlay network一般要比物理网络简单。现在类似P2P这样的覆盖网络在复杂性上还不能与互联网相提并论。基础网络面向大多数应用，提供的是普遍服务，因此具有较高的复杂性，而Overlay network只服务于特定的业务，因此不会有复杂的结构。

简单说来覆盖网络就是应用层网络，面向应用层，不考虑或很少考虑网络层，物理层的问题。Overlay network的主要作用是把特定的问题从互联网无穷多的目标优化问题中剥离出来，映射到一个独立的空间，进行高效的解决。例如在研究领域棘手的广域网视频传输质量问题，就被覆盖网络轻松解决了。

由此可见，Overlay network主要是对物理网络的一个补强，其结构相对简单，但是却对一些传统物理网络难以解决的问题具有极强的适用性。然而其也无法像物理网络一样面向大多数的网络应用，而是作为特殊手段运用于特殊场景。

~~~python
import numpy as np
import matplotlib.pylab as plt

x = np.loadtxt('C:\\Users\\26082\\Desktop\\code\\生成数据\\data_X.txt', delimiter=',')
y = np.loadtxt('C:\\Users\\26082\\Desktop\\code\\生成数据\\data_Y.txt', delimiter=',')
y1 = np.sin(x)

lambd = 0.02


def gradient(W, X, T):
    '''在带正则项的最小二乘函数中计算W处的梯度'''
    return (1 / X.shape[1]) * (X.T * X * W - X.T * T + lambd * W)


def gradient_descent(X, T, learning_rate, times):
    '''进行梯度下降的迭代'''
    # 初始化W为全为0的列向量
    W = np.mat(np.zeros(X.shape[1])).T
    now = 0
    grad = gradient(W, X, T)
    while np.all(np.absolute(grad) > 1e-9):
        W = W - learning_rate * grad
        # print(W)
        grad = gradient(W, X, T)
        now += 1
        if now > times:
            print("超出次数" + str(now))
            break
    print("次数" + str(now))
    return X.dot(W)


def gradient_descent01(X, T, learning_rate):
    '''进行梯度下降的迭代'''
    # 初始化W为全为0的列向量
    W = np.mat(np.zeros(X.shape[1])).T
    now = 0
    grad = gradient(W, X, T)
    while np.all(np.absolute(grad) > 1e-7):
        W = W - learning_rate * grad
        # print(W)
        grad = gradient(W, X, T)
        now += 1
    print("次数" + str(now))
    return X.dot(W)


m = []
for i in range(6):
    a = x ** i
    m.append(a)

A = np.mat(m).T
b = np.mat(y).T

# yw = gradient_descent(A, b, 0.000061, 200000)
yw = gradient_descent01(A, b, 0.0000001001)

# print(yw)
yw.shape = (yw.shape[0],)
# print(yw)

a = yw.flatten().A[0]

print(a)
plt.plot(x, y1, color='g', linestyle='-', marker='', label=u"理想曲线")
plt.plot(x, y, color='m', linestyle='', marker='o', label=u"已知数据点")
plt.plot(x, a, color='r', linestyle='-', marker='.', label=u"拟合曲线")
plt.show()
~~~

